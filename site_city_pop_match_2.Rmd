---
title: "Site Population Matching - Small Batch"
date: "September 2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(stringi)
library(tidycensus)
library(sf)
library(tigris)
library(dplyr)

```

This document has an alternative use case from the more comprehensive site_city_pop_mathc.Rmd. This notebook's purpose is to allow for anyone to come in with a handful of new sites that they want to get population data for. It still requires that we have some data sets installed from the census, as well as an API key. It also will require manual input for some attributes for each of the sites that we are interested in finding this information for. This file will not write output, so any results you would like to record from your analysis will need to be saved separately.

The next two chunks will require some user input. This will help to identify the sites you want data for, and to direct the script to where the other data sources are on your computer.

```{r}
# Set up an empty data frame
new_sites <- data.frame(Site_City = character(),
                        State_Name = character(),
                        lat = numeric(),
                        lon = numeric(),
                        stringsAsFactors = FALSE)

# Here is an example of how to add sites you want to get data for
# If you have multiple sites, simply copy and paste the line below and add the new information
new_sites[nrow(new_sites) + 1, ] <- c('Santa Barbara', 'California', 34.41896, -119.69961)

```



```{r}

# Get your own API key here: https://api.census.gov/data/key_signup.html
census_api_key('YOUR_KEY_HERE', install = TRUE, overwrite = TRUE)

# UPDATE this with the location of your data
data_path <- 'data/raw'

# Downloaded from Census under 'Datasets' tab https://www.census.gov/data/tables/time-series/demo/popest/2020s-total-cities-and-towns.html
city_pop <- read.csv(file.path(data_path, 'sub-est2023.csv'))

# Downloaded from Census https://data.census.gov/table?q=Annual%20Estimates%20of%20the%20Resident%20Population&g=010XX00US$1600000
places_pop <- read.csv(file.path(data_path, 'DECENNIALDP2020.DP1-Data.csv'))

# Downloaded from SimpleMaps https://simplemaps.com/data/us-neighborhoods
neighborhood_link <- read.csv(file.path(data_path, 'usneighborhoods.csv'))

```


```{r, warning = FALSE}
# Format data to allow for matching based on city name
places_pop_clean <- places_pop %>%
  select(GEO_ID, NAME, DP1_0001C) %>%
  mutate(DP1_0001C = as.numeric(DP1_0001C)) %>% # Make population column numeric
  filter(!str_detect(NAME, '(pt.)'), # Removes partial city data
         !str_detect(NAME, 'township')) %>% # Removes townships that overlap with cities
  separate(NAME, into = c('Site_City', 'State_Name'), sep = ", ", extra = "merge", fill = "right") %>%
  mutate(Site_City = stri_replace_all_regex(stri_trans_nfd(Site_City), "\\p{Mn}", ""), # Removes special characters from PR cities
         Site_City = str_replace(Site_City, ' city.*', ''), # Remove unnecessary name endings
         Site_City = str_replace(Site_City, ' town.*', ''),
         Site_City = str_replace(Site_City, ' village.*', ''),
         Site_City = str_replace(Site_City, ' borough.*', ''),
         Site_City = str_replace(Site_City, ' County.*', ''),
         Site_City = str_replace(Site_City, ' municipality.*', ''),
         Site_City = str_replace(Site_City, 'St\\.', 'Saint'),
         Site_City = str_replace(Site_City, ' CDP.*', ''),
         Site_City = str_replace(Site_City, "'", ""), # Remove apostrophes
         Site_City = str_replace(Site_City, ' zona urbana', ''),
         Site_City = str_replace(Site_City, ' comunidad', ''),
         Site_City = ifelse(State_Name %in% c('Georgia', 'Kentucky', 'North Carolina', 
                                              'Montana', 'Tennessee'), 
                            str_replace(Site_City, '-.*', ''), Site_City), # Remove extra long endings after hyphens
         
         Site_City = str_replace(Site_City, 'Coeur dAlene', 'coeur d alene'), # Specific instance in Idaho
         Site_City = str_replace(Site_City, 'El Paso de.*', 'paso robles'), # Specific instance in California
         Site_City = tolower(Site_City)) %>%
  select(Site_City, State_Name, Population = DP1_0001C) %>% # Take specific columns and rename population
  group_by(Site_City, State_Name) %>%
  summarize(across(c(Population), max)) %>% # Simplify results if there are duplicates, taking the max population
  ungroup()

# Format data to allow for matching based on city name
city_pop_clean <- city_pop %>%
  filter(FUNCSTAT %in% c('A', 'B', 'C'), # Ensures we only get active cities
         !str_detect(NAME, '(pt.)'), # Removes partial city data
         !str_detect(NAME, 'township')) %>% # Removes townships that overlap with cities
  select(Site_City = NAME, State_Name = STNAME, Population = ESTIMATESBASE2020) %>%
  mutate(Site_City = stri_replace_all_regex(stri_trans_nfd(Site_City), "\\p{Mn}", ""), # Removes special characters from PR cities
         Site_City = str_replace(Site_City, ' city.*', ''), # Remove unnecessary name endings
         Site_City = str_replace(Site_City, ' town.*', ''),
         Site_City = str_replace(Site_City, ' village.*', ''),
         Site_City = str_replace(Site_City, ' borough.*', ''),
         Site_City = str_replace(Site_City, ' County.*', ''),
         Site_City = str_replace(Site_City, ' municipality.*', ''),
         Site_City = str_replace(Site_City, 'St\\.', 'Saint'),
         Site_City = str_replace(Site_City, "'", ""), # Remove apostrophes
         Site_City = str_replace(Site_City, ' zona urbana', ''),
         Site_City = str_replace(Site_City, ' comunidad', ''),
         Site_City = ifelse(State_Name %in% c('Georgia', 'Kentucky', 'North Carolina', 
                                              'Montana', 'Tennessee'), 
                            str_replace(Site_City, '-.*', ''), Site_City),
         Site_City = str_replace(Site_City, "Coeur dAlene", "coeur d alene"), # Specific instance in Idaho
         Site_City = str_replace(Site_City, "El Paso de.*", "paso robles"), # Specific instance in California
         Site_City = tolower(Site_City)) %>% # Join above observations
  group_by(Site_City, State_Name) %>%
  summarize(across(c(Population), max)) %>% # Simplify results if there are duplicates
  ungroup()

new_sites <- new_sites %>%
  mutate(Site_City = tolower(Site_City))

new_sites_pop <- left_join(new_sites, places_pop_clean, by = c('State_Name', 'Site_City')) %>% # Join data by state and city
  select(Site_City, State_Name, Population, lat, lon) %>%
  left_join(city_pop_clean, by = c('State_Name', 'Site_City')) %>% # Join secondary data set by state and city
  mutate(Population.x = ifelse(!is.na(Population.y), Population.y, Population.x)) %>% # Update empty population entries with new values 
  select(Site_City, State_Name, Population = Population.x, lat, lon)

# If the above methods don't produce results for every site, this will run
if (sum(is.na(new_sites_pop$Population)) != 0){
  new_sites_pop_missing <-  new_sites_pop[is.na(new_sites_pop[['Population']]), ] %>% # Filter for missing populations
  mutate(location_full = paste(Site_City, ', ', State_Name, sep = '')) # Create new variable for easy manual searching
  
  neighborhood_link_clean <- neighborhood_link %>%
    select(Site_City = neighborhood, State_Name = state_name, Updated_City = city_name) %>% # Rename columns
    mutate(Site_City = tolower(Site_City), # Make column entries lowercase so they'll match up with other data
           Updated_City = tolower(Updated_City))
  
  updated_locations <- left_join(new_sites_pop_missing, neighborhood_link_clean, by = c('State_Name', 'Site_City')) # Connect some missing sites to cities
  
  if (sum(!is.na(updated_locations$Updated_City)) != 0){
    new_sites_pop <- new_sites_pop %>%
      left_join(updated_locations, by = c('State_Name', 'Site_City')) %>% # Use state and city to link updated locations
      mutate(Site_City = ifelse(!is.na(Updated_City), Updated_City, Site_City)) %>% # Updated site city locations with new finds
      select(Site_City, State_Name, lat = lat.x, lon = lon.x) %>% # Rename columns
      left_join(places_pop_clean, by = c('State_Name', 'Site_City')) %>% # Join data by state and city
      select(Site_City, State_Name, Population, lat, lon) %>%
      left_join(city_pop_clean, by = c('State_Name', 'Site_City')) %>% # Join secondary data set by state and city
      mutate(Population.x = ifelse(!is.na(Population.y), Population.y, Population.x)) %>% # Update empty population entries with new values 
      select(Site_City, State_Name, Population = Population.x, lat, lon) # Remove unnecessary columns
  }
  
  # Create a subset of remaining missing sites
  missing_locs <- new_sites_pop[is.na(new_sites_pop$Population), ] 
  
  # Create spatial object with site coordinates
  coords_sf <- st_as_sf(missing_locs, coords = c("lat", "lon"), crs = 4326) 
  
  # Make a list of all 50 states, D.C., and Puerto Rico
  states <- unique(fips_codes$state)[1:55] 
  
  # Aggregate Census tract shapefiles
  tracts_list <- map(states, ~tracts(state = .x, year = 2020, class = "sf")) 
  
  # Combine all states' tracts into one sf object
  all_tracts <- do.call(rbind, tracts_list) %>% 
    st_transform(all_tracts, crs = 4326) # Set matching coordinate system
  
  all_tracts <- st_make_valid(all_tracts)
  coords_sf <- st_make_valid(coords_sf)
  
  # Spatial join coordinates with tracts to get the GEOID
  coords_with_geoid <- st_join(coords_sf, all_tracts, join = st_intersects) %>% distinct() 
  
  # Get population data for each unique GEOID through the Census American Community Survey
  population_data <- map_dfr(unique(coords_with_geoid$GEOID), 
                             ~get_acs(geography = "tract", 
                                      variables = "B01003_001", 
                                      year = 2020, 
                                      state = substr(.x, 1, 2), 
                                      tract = substr(.x, 3, 11)))
  
  # Join the population data back to original coordinates
  missing_locs_found <- coords_with_geoid %>%
    st_drop_geometry() %>% # Not needed anymore
    left_join(population_data, by = "GEOID") %>% # Link sites to population values
    distinct() %>% # Remove duplicates if they occur
    select(Site_City, State_Name, estimate)
  
  # Add newly found values to ongoing data
  new_sites_pop <- new_sites_pop %>%
    left_join(missing_locs_found, by = c('State_Name', 'Site_City')) %>%
    mutate(Population = ifelse(!is.na(estimate), estimate, Population)) %>% # Update formerly missing populations
    select(-estimate) %>% # Remove unnecessary column
    mutate(rural_indicator = Population < 10000, # Create indicator column
           community_rural_doe = case_when(rural_indicator == TRUE ~ 'Yes',
                                           rural_indicator == FALSE ~ 'No'))
}

print(new_sites_pop)


```
